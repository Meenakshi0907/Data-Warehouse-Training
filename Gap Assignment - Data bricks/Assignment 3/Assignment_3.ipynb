{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "RmV4GcO9ow0c",
        "outputId": "6266427e-a8bf-4a0f-e391-d5dc8d8df951"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bab07bd30d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0b870c85c362:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>assignment 3</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        ".appName(\"assignment 3\")\\\n",
        ".getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task Set â€“ Intermediate to Advanced PySpark (No DLT)**"
      ],
      "metadata": {
        "id": "LfZpyBkMvacH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Ingestion & Schema Handling**"
      ],
      "metadata": {
        "id": "-90OIRwtvdto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the CSV using inferred schema.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_emp_ts = spark.read.csv('/content/drive/MyDrive/employee_timesheet.csv',header= True,inferSchema=True)\n",
        "df_emp_ts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PkafoGMuyqn",
        "outputId": "834712e3-00d4-4316-dbd9-cf2348a0c463"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the same file with schema explicitly defined.\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"EmployeeID\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Department\", StringType(), True),\n",
        "    StructField(\"Project\", StringType(), True),\n",
        "    StructField(\"WorkHours\", IntegerType(), True),\n",
        "    StructField(\"WorkDate\", DateType(), True),\n",
        "    StructField(\"Location\", StringType(), True),\n",
        "    StructField(\"Mode\", StringType(), True),\n",
        "])\n",
        "\n",
        "df_emp_ts = spark.read.option(\"header\", True).schema(schema).csv(\"/content/drive/MyDrive/employee_timesheet.csv\")\n",
        "df_emp_ts.show()\n",
        "df_emp_ts.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb81MFy9vwgK",
        "outputId": "a79cae4f-8ecf-407b-ba51-202d2e127f4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- EmployeeID: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Project: string (nullable = true)\n",
            " |-- WorkHours: integer (nullable = true)\n",
            " |-- WorkDate: date (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Mode: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Add a new column Weekday extracted from WorkDate\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "df_with_weekday = df_emp_ts.withColumn(\"Weekday\", date_format(\"WorkDate\", \"EEEE\"))\n",
        "df_with_weekday.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LCV3v9AwMHV",
        "outputId": "d806f369-cadc-49ee-e3e5-b1e9db17f29d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggregations & Grouping**"
      ],
      "metadata": {
        "id": "qguFf9znza_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Calculate total work hours by employee.\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df_total_hours = df_emp_ts.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\"))\n",
        "df_total_hours.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA5YXcGDzU9Q",
        "outputId": "b49151c8-84d7-4758-e943-8808389fc080"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+\n",
            "|EmployeeID| Name|TotalHours|\n",
            "+----------+-----+----------+\n",
            "|      E103| John|         5|\n",
            "|      E104|Meena|         6|\n",
            "|      E102|  Raj|        15|\n",
            "|      E101|Anita|        17|\n",
            "+----------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Calculate average work hours per department.\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df_avg_dept = df_emp_ts.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AvgHours\"))\n",
        "df_avg_dept.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7gTC4Nw9i6W",
        "outputId": "ba6623e9-7ba3-4450-940b-1312f8443725"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|Department|         AvgHours|\n",
            "+----------+-----------------+\n",
            "|        HR|              7.5|\n",
            "|   Finance|              5.0|\n",
            "|        IT|7.666666666666667|\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Get top 2 employees by total hours using window function.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "window_spec = Window.orderBy(df_total_hours[\"TotalHours\"].desc())\n",
        "\n",
        "df_top_2 = df_total_hours.withColumn(\"rank\", row_number().over(window_spec)).filter(\"rank <= 2\")\n",
        "df_top_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szRM7kDj9lit",
        "outputId": "4d6ac8a2-5a8e-46f8-91fc-48c4d2e7f56e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+----+\n",
            "|EmployeeID| Name|TotalHours|rank|\n",
            "+----------+-----+----------+----+\n",
            "|      E101|Anita|        17|   1|\n",
            "|      E102|  Raj|        15|   2|\n",
            "+----------+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Date Operations**"
      ],
      "metadata": {
        "id": "Cmc_LMDx-AgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Filter entries where WorkDate falls on a weekend.\n",
        "from pyspark.sql.functions import dayofweek\n",
        "\n",
        "df_weekends = df_emp_ts.filter(dayofweek(\"WorkDate\").isin([1, 7]))\n",
        "df_weekends.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XosxGg3L-J48",
        "outputId": "ce9363b2-36aa-4fef-e417-eb3fe1b1c6bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+-------+---------+----------+--------+------+\n",
            "|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location|  Mode|\n",
            "+----------+----+----------+-------+---------+----------+--------+------+\n",
            "|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|\n",
            "+----------+----+----------+-------+---------+----------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Calculate running total of hours per employee using window.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "window_spec_emp = Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "df_running_total = df_emp_ts.withColumn(\"RunningTotalHours\", sum(\"WorkHours\").over(window_spec_emp))\n",
        "df_running_total.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRlCNUrF-NEY",
        "outputId": "64a0d6ab-3352-4700-d917-ea758aa03cfb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------+-----------------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|RunningTotalHours|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+-----------------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|                8|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|               17|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|                7|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|               15|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|                5|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|                6|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Joining DataFrames**\n"
      ],
      "metadata": {
        "id": "DgQv87W7-5dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Create department_location.csv :\n",
        "# Department,DeptHead\n",
        "# IT,Anand\n",
        "# HR,Shruti\n",
        "# Finance,Kamal\n",
        "\n",
        "df_dep_ts = spark.read.csv('/content/drive/MyDrive/department_location.csv',header= True,inferSchema=True)\n",
        "df_dep_ts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npJSBQQz-4s6",
        "outputId": "6609b256-534f-4fb5-a889-055963d7913d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|Department|DeptHead|\n",
            "+----------+--------+\n",
            "|        IT|   Anand|\n",
            "|        HR|  Shruti|\n",
            "|   Finance|   Kamal|\n",
            "+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Join with timesheet data and list all employees with their DeptHead.\n",
        "df_joined = df_emp_ts.join(df_dep_ts, on=\"Department\", how=\"left\")\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7gVrh7u_IuQ",
        "outputId": "9a2c8126-2ae0-446f-aa93-a0270884779a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+-------+---------+----------+---------+------+--------+\n",
            "|Department|EmployeeID| Name|Project|WorkHours|  WorkDate| Location|  Mode|DeptHead|\n",
            "+----------+----------+-----+-------+---------+----------+---------+------+--------+\n",
            "|        IT|      E101|Anita|  Alpha|        8|2024-05-01|Bangalore|Remote|   Anand|\n",
            "|        HR|      E102|  Raj|   Beta|        7|2024-05-01|   Mumbai|Onsite|  Shruti|\n",
            "|   Finance|      E103| John|  Alpha|        5|2024-05-02|    Delhi|Remote|   Kamal|\n",
            "|        IT|      E101|Anita|  Alpha|        9|2024-05-03|Bangalore|Remote|   Anand|\n",
            "|        IT|      E104|Meena|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Anand|\n",
            "|        HR|      E102|  Raj|   Beta|        8|2024-05-04|   Mumbai|Remote|  Shruti|\n",
            "+----------+----------+-----+-------+---------+----------+---------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pivot & Unpivot**"
      ],
      "metadata": {
        "id": "SMa9gurNAaM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Pivot table: total hours per employee per project.\n",
        "\n",
        "df_pivot = df_emp_ts.groupBy(\"EmployeeID\", \"Name\").pivot(\"Project\").sum(\"WorkHours\")\n",
        "df_pivot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnE417r7AZap",
        "outputId": "51765f8c-c61a-44a6-f08e-5b953a9716df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+----+-----+\n",
            "|EmployeeID| Name|Alpha|Beta|Gamma|\n",
            "+----------+-----+-----+----+-----+\n",
            "|      E103| John|    5|NULL| NULL|\n",
            "|      E104|Meena| NULL|NULL|    6|\n",
            "|      E102|  Raj| NULL|  15| NULL|\n",
            "|      E101|Anita|   17|NULL| NULL|\n",
            "+----------+-----+-----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Unpivot example: Convert mode-specific hours into rows.\n",
        "from pyspark.sql.functions import when,expr\n",
        "\n",
        "df_mode_hours = df_emp_ts.withColumn(\"RemoteHours\", when(df_emp_ts[\"Mode\"] == \"Remote\", df_emp_ts[\"WorkHours\"]).otherwise(0))\\\n",
        ".withColumn(\"OnsiteHours\", when(df_emp_ts[\"Mode\"] == \"Onsite\", df_emp_ts[\"WorkHours\"]).otherwise(0))\n",
        "\n",
        "df_unpivoted = df_mode_hours.select(\"EmployeeID\", \"Name\", \"WorkDate\",\n",
        "    expr(\"stack(2, 'Remote', RemoteHours, 'Onsite', OnsiteHours) as (ModeType, Hours)\")\n",
        ")\n",
        "df_unpivoted.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx8XCo65Ah2B",
        "outputId": "d065985f-d693-4d53-b4b0-eb50002ce701"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+--------+-----+\n",
            "|EmployeeID| Name|  WorkDate|ModeType|Hours|\n",
            "+----------+-----+----------+--------+-----+\n",
            "|      E101|Anita|2024-05-01|  Remote|    8|\n",
            "|      E101|Anita|2024-05-01|  Onsite|    0|\n",
            "|      E102|  Raj|2024-05-01|  Remote|    0|\n",
            "|      E102|  Raj|2024-05-01|  Onsite|    7|\n",
            "|      E103| John|2024-05-02|  Remote|    5|\n",
            "|      E103| John|2024-05-02|  Onsite|    0|\n",
            "|      E101|Anita|2024-05-03|  Remote|    9|\n",
            "|      E101|Anita|2024-05-03|  Onsite|    0|\n",
            "|      E104|Meena|2024-05-03|  Remote|    0|\n",
            "|      E104|Meena|2024-05-03|  Onsite|    6|\n",
            "|      E102|  Raj|2024-05-04|  Remote|    8|\n",
            "|      E102|  Raj|2024-05-04|  Onsite|    0|\n",
            "+----------+-----+----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UDF & Conditional Logic**\n"
      ],
      "metadata": {
        "id": "xvxwFRULBsPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Create a UDF to classify work hours:\n",
        "# def workload_tag(hours):\n",
        "# if hours >= 8: return \"Full\"\n",
        "# elif hours >= 4: return \"Partial\"\n",
        "# else: return \"Light\"\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def workload_tag(hours):\n",
        "    if hours >= 8:\n",
        "        return \"Full\"\n",
        "    elif hours >= 4:\n",
        "        return \"Partial\"\n",
        "    else:\n",
        "        return \"Light\"\n",
        "\n",
        "workload_udf = udf(workload_tag, StringType())"
      ],
      "metadata": {
        "id": "ujWMf768BdmJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Add a column WorkloadCategory using this UDF.\n",
        "\n",
        "df_with_category = df_emp_ts.withColumn(\"WorkloadCategory\", workload_udf(\"WorkHours\"))\n",
        "df_with_category.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW8JYMS_ByTO",
        "outputId": "06b5ff0d-b7c5-4357-cd0b-40bf4f64c164"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------+----------------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|WorkloadCategory|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+----------------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|            Full|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|         Partial|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|         Partial|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|            Full|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|         Partial|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|            Full|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nulls and Cleanup**\n"
      ],
      "metadata": {
        "id": "dj6NV328CRsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Introduce some nulls in Mode column.\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_with_nulls = df_emp_ts.withColumn(\"Mode\",when(df_emp_ts[\"EmployeeID\"] == \"E102\", None).otherwise(df_emp_ts[\"Mode\"]))\n",
        "df_with_nulls.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FgxRFZvCL-1",
        "outputId": "029eae66-a318-4f38-9ddf-ef417a96ba1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|  NULL|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|  NULL|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Fill nulls with \"Not Provided\".\n",
        "\n",
        "df_filled = df_with_nulls.fillna({\"Mode\": \"Not Provided\"})\n",
        "df_filled.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9ZrNO8PCW4H",
        "outputId": "ec9ff4fc-e41d-4784-f744-73e7fcd5e8e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Not Provided|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Not Provided|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Drop rows where WorkHours < 4.\n",
        "\n",
        "df_cleaned = df_filled.filter(df_filled[\"WorkHours\"] >= 4)\n",
        "df_cleaned.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMJfX7dKCYwg",
        "outputId": "a6d18431-6a9f-4ee7-a368-07e5931fd155"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Not Provided|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Not Provided|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Conditions**"
      ],
      "metadata": {
        "id": "jw_nWsRrC5Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries are Remote.\n",
        "\n",
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "df_remote_stats = df_emp_ts.groupBy(\"EmployeeID\").agg(count(\"*\").alias(\"TotalEntries\"),count(when(col(\"Mode\") == \"Remote\", True)).alias(\"RemoteEntries\"))\n",
        "\n",
        "from pyspark.sql.functions import expr, when\n",
        "\n",
        "df_remote_flagged = df_remote_stats.withColumn(\"RemoteRatio\", col(\"RemoteEntries\") / col(\"TotalEntries\")).withColumn(\n",
        "    \"RemoteTag\", when(col(\"RemoteRatio\") > 0.8, \"Remote Worker\").otherwise(\"Hybrid/Onsite\"))\n",
        "\n",
        "df_remote_flagged.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hGRxKnbC_Z7",
        "outputId": "947f9220-4a80-437d-aca8-3faf96f3cb89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+-----------+-------------+\n",
            "|EmployeeID|TotalEntries|RemoteEntries|RemoteRatio|    RemoteTag|\n",
            "+----------+------------+-------------+-----------+-------------+\n",
            "|      E103|           1|            1|        1.0|Remote Worker|\n",
            "|      E104|           1|            0|        0.0|Hybrid/Onsite|\n",
            "|      E101|           2|            2|        1.0|Remote Worker|\n",
            "|      E102|           2|            1|        0.5|Hybrid/Onsite|\n",
            "+----------+------------+-------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Add a new column ExtraHours where hours > 8.\n",
        "\n",
        "df_with_extra = df_emp_ts.withColumn(\"ExtraHours\", when(col(\"WorkHours\") > 8, col(\"WorkHours\") - 8).otherwise(0))\n",
        "df_with_extra.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epeIKC6vDdSd",
        "outputId": "e4c312be-2a03-4f46-8308-daf22285739e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+---------+----------+---------+------+----------+\n",
            "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|ExtraHours|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+----------+\n",
            "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|         0|\n",
            "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|         0|\n",
            "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|         0|\n",
            "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|         1|\n",
            "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|         0|\n",
            "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|         0|\n",
            "+----------+-----+----------+-------+---------+----------+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Union + Duplicate Handling**\n"
      ],
      "metadata": {
        "id": "tpWEZXbBDtl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Append a dummy timesheet for new interns using unionByName() .\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "intern_data = [\n",
        "    Row(EmployeeID=\"E201\", Name=\"Ishaan\", Department=\"IT\", Project=\"Gamma\", WorkHours=6, WorkDate=\"2024-05-05\", Location=\"Chennai\", Mode=\"Remote\"),\n",
        "    Row(EmployeeID=\"E202\", Name=\"Nitya\", Department=\"HR\", Project=\"Beta\", WorkHours=5, WorkDate=\"2024-05-05\", Location=\"Pune\", Mode=\"Onsite\")\n",
        "]\n",
        "\n",
        "df_interns = spark.createDataFrame(intern_data).withColumn(\"WorkDate\", col(\"WorkDate\").cast(\"date\"))\n",
        "\n",
        "df_combined = df_emp_ts.unionByName(df_interns)\n",
        "df_combined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itgkHMtPDxPW",
        "outputId": "d5bbd489-b9b9-4d6a-ec85-aebe0389b6d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+-------+---------+----------+---------+------+\n",
            "|EmployeeID|  Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n",
            "+----------+------+----------+-------+---------+----------+---------+------+\n",
            "|      E101| Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n",
            "|      E102|   Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n",
            "|      E103|  John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n",
            "|      E101| Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n",
            "|      E104| Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n",
            "|      E102|   Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n",
            "|      E201|Ishaan|        IT|  Gamma|        6|2024-05-05|  Chennai|Remote|\n",
            "|      E202| Nitya|        HR|   Beta|        5|2024-05-05|     Pune|Onsite|\n",
            "+----------+------+----------+-------+---------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Remove duplicate rows based on all columns.\n",
        "\n",
        "df_deduplicated = df_combined.dropDuplicates()\n",
        "df_deduplicated.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI3brdD3DzaB",
        "outputId": "47618807-c9bf-4fcd-a25d-3ac4a3992388"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+-------+---------+----------+---------+------+\n",
            "|EmployeeID|  Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n",
            "+----------+------+----------+-------+---------+----------+---------+------+\n",
            "|      E104| Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n",
            "|      E101| Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n",
            "|      E103|  John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n",
            "|      E101| Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n",
            "|      E102|   Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n",
            "|      E102|   Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n",
            "|      E201|Ishaan|        IT|  Gamma|        6|2024-05-05|  Chennai|Remote|\n",
            "|      E202| Nitya|        HR|   Beta|        5|2024-05-05|     Pune|Onsite|\n",
            "+----------+------+----------+-------+---------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}