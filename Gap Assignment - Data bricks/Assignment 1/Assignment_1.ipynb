{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPfVGQ-nSLBf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Assignment1\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Ingest all 3 CSVs as Delta Tables\n",
        "# Load as DataFrames\n",
        "df_orders = spark.read.option(\"header\", True).option(\"inferSchema\", True) \\\n",
        "    .csv(\"file:/Workspace/Shared/orders.csv\")\n",
        "\n",
        "df_customers = spark.read.option(\"header\", True).option(\"inferSchema\", True) \\\n",
        "    .csv(\"file:/Workspace/Shared/customers.csv\")\n",
        "\n",
        "df_products = spark.read.option(\"header\", True).option(\"inferSchema\", True) \\\n",
        "    .csv(\"file:/Workspace/Shared/products.csv\")\n",
        "\n",
        "# Save as Delta\n",
        "df_orders.write.mode(\"overwrite\").format(\"delta\").save(\"/Workspace/Shared/delta/orders\")\n",
        "df_customers.write.mode(\"overwrite\").format(\"delta\").save(\"/Workspace/Shared/delta/customers\")\n",
        "df_products.write.mode(\"overwrite\").format(\"delta\").save(\"/Workspace/Shared/delta/products\")\n",
        "#show the tables\n",
        "df_customers.show()\n",
        "df_products.show()\n",
        "df_orders.show()"
      ],
      "metadata": {
        "id": "r0Pwyz2uSR4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Total Revenue per Product\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "df_revenue_product = df_orders.withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")).groupBy(\"ProductID\").agg(sum(\"Revenue\").alias(\"TotalRevenue\"))\n",
        "\n",
        "df_revenue_product.show()"
      ],
      "metadata": {
        "id": "moF_7VrISfko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Revenue by Region (Join Orders + Customers)\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "df_joined = df_orders.join(df_customers, \"CustomerID\")\n",
        "\n",
        "df_revenue_region = df_joined.withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")).groupBy(\"Region\").agg(sum(\"Revenue\").alias(\"TotalRevenue\"))\n",
        "df_revenue_region.show()"
      ],
      "metadata": {
        "id": "5o-OhYmOStY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Update Status of Pending Orders to 'Cancelled'\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "orders_table = DeltaTable.forPath(spark, \"/Workspace/Shared/delta/orders\")\n",
        "\n",
        "orders_table.update(condition=expr(\"Status = 'Pending'\"),set={\"Status\": expr(\"'Cancelled'\")})\n",
        "spark.read.format('delta').load('/Workspace/Shared/delta/orders').show()"
      ],
      "metadata": {
        "id": "zd-zHO_MSzAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #5. Merge a New Return Record into Orders\n",
        "from pyspark.sql import Row\n",
        "\n",
        "new_return = [Row(OrderID=3006, CustomerID='C002', ProductID='P1003', Quantity=1, Price=30000, OrderDate='2024-05-06', Status='Returned')]\n",
        "\n",
        "df_new = spark.createDataFrame(new_return)\n",
        "\n",
        "orders_table.alias(\"target\").merge(df_new.alias(\"source\"),\"target.OrderID = source.OrderID\").whenNotMatchedInsertAll().execute()\n",
        "df_new.show()"
      ],
      "metadata": {
        "id": "u-YzCmirTYDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. DLT Pipeline Simulation: Raw → Cleaned → Aggregated\n",
        "df_orders_raw = spark.read.format(\"delta\").load(\"/Workspace/Shared/delta/orders\")\n",
        "\n",
        "# Cleaned table: remove rows with any NULLs\n",
        "df_cleaned = df_orders_raw.na.drop()\n",
        "df_cleaned.show()\n",
        "# 11Optionally register as temp view if needed later\n",
        "df_cleaned.createOrReplaceTempView(\"orders_cleaned\")\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "df_products = spark.read.format(\"delta\").load(\"/Workspace/Shared/delta/products\")\n",
        "\n",
        "df_cleaned = spark.read.table(\"orders_cleaned\")\n",
        "\n",
        "df_joined = df_cleaned.join(df_products, on=\"ProductID\")\n",
        "\n",
        "df_category_revenue = df_joined.withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")).groupBy(\"Category\").agg(sum(\"Revenue\").alias(\"TotalRevenue\"))\n",
        "\n",
        "df_category_revenue.show()"
      ],
      "metadata": {
        "id": "-v9RMpQLTguC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. View Data Before the Status Update (Time Travel)\n",
        "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/Workspace/Shared/delta/orders\")\n",
        "\n",
        "df_v0.select(\"OrderID\", \"Status\").show()"
      ],
      "metadata": {
        "id": "CaPjd2ovTkar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Restore to an Older Version (e.g., version 0)\n",
        "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/Workspace/Shared/delta/orders\")\n",
        "\n",
        "df_old.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"/Workspace/Shared/delta/orders\")\n",
        "df_old.show()"
      ],
      "metadata": {
        "id": "-tF_cr8bVFjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. VACUUM with Shortened Retention Period\n",
        "\n",
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
        "\n",
        "spark.sql(\"VACUUM delta.`/Workspace/Shared/delta/orders` RETAIN 0 HOURS\")"
      ],
      "metadata": {
        "id": "s7A2JP2MVM-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Expectations (Validation)\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_orders = spark.read.format(\"delta\").load(\"/Workspace/Shared/delta/orders\")\n",
        "\n",
        "df_valid = df_orders.filter((col(\"Quantity\") > 0) & (col(\"Price\") > 0) & (col(\"OrderDate\").isNotNull())\n",
        "\n",
        "df_valid.show()"
      ],
      "metadata": {
        "id": "rSW_VqiYVS9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Bonus – OrderType Column with when-otherwise\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_orders = spark.read.format(\"delta\").load(\"/Workspace/Shared/delta/orders\")\n",
        "\n",
        "df_with_order_type = df_orders.withColumn(\n",
        "    \"OrderType\",\n",
        "    when(col(\"Status\") == \"Returned\", \"Return\").otherwise(\"Regular\")\n",
        ")\n",
        "\n",
        "df_with_order_type.select(\"OrderID\", \"Status\", \"OrderType\").show()"
      ],
      "metadata": {
        "id": "WVLVdyWpVVcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}