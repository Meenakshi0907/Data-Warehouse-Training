{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "RIuKgAikIQKe",
        "outputId": "467c80d0-8a5c-4dce-bb62-87331c867fce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b09fff2a850>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9913cedceb89:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pysparkandsql</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        ".appName(\"pysparkandsql\")\\\n",
        ".getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCJtBpiOKGOa",
        "outputId": "8fd4124a-9775-430d-f5dd-8210f73774fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "# Customers Data\n",
        "customers_data = [\n",
        "(101, 'Ali', 'ali@gmail.com', 'Mumbai', '2022-05-10'),\n",
        "(102, 'Neha', 'neha@yahoo.com', 'Delhi', '2023-01-15'),\n",
        "(103, 'Ravi', 'ravi@hotmail.com', 'Bangalore', '2021-11-01'),\n",
        "(104, 'Sneha', 'sneha@outlook.com', 'Hyderabad', '2020-07-22'),\n",
        "(105, 'Amit', 'amit@gmail.com', 'Chennai', '2023-03-10'),\n",
        "]\n",
        "orders_data = [\n",
        "(1, 101, 'Laptop', 'Electronics', 2, 50000.0, '2024-01-10'),\n",
        "(2, 101, 'Mouse', 'Electronics', 1, 1200.0, '2024-01-15'),\n",
        "(3, 102, 'Tablet', 'Electronics', 1, 20000.0, '2024-02-01'),\n",
        "(4, 103, 'Bookshelf', 'Furniture', 1, 3500.0, '2024-02-10'),\n",
        "(5, 104, 'Mixer', 'Appliances', 1, 5000.0, '2024-02-15'),\n",
        "(6, 105, 'Notebook', 'Stationery', 5, 500.0, '2024-03-01'),\n",
        "(7, 102, 'Phone', 'Electronics', 1, 30000.0, '2024-03-02'),\n",
        "]\n",
        "customers_df = spark.createDataFrame(customers_data, [\"CustomerID\", \"Name\", \"Email\",\"City\", \"SignupDate\"])\n",
        "orders_df = spark.createDataFrame(orders_data, [\"OrderID\", \"CustomerID\", \"Product\",\"Category\", \"Quantity\", \"Price\", \"OrderDate\"])\n",
        "# Write as tables\n",
        "\n",
        "customers_df.write.mode(\"overwrite\").saveAsTable(\"sales.customers\")\n",
        "orders_df.write.mode(\"overwrite\").saveAsTable(\"sales.orders\")"
      ],
      "metadata": {
        "id": "TTvSXD8tI8b9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION A: PySpark DataFrame Tasks\n",
        "# 1. Add a column TotalAmount = Price * Quantity to the orders_df .\n",
        "\n",
        "orders_df = orders_df.withColumn(\"TotalAmount\",orders_df[\"Price\"]*orders_df[\"Quantity\"])\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5IoEYPMKKm-",
        "outputId": "a4fa49d7-c3f4-472d-932c-00e6a13b23bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Filter all orders with TotalAmount > 10000 .\n",
        "\n",
        "higher_amount = orders_df.filter(orders_df[\"TotalAmount\"] > 10000)\n",
        "higher_amount.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUyCUfTPKzgo",
        "outputId": "17c8781f-9110-4f14-c2d9-f7178866a2ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+\n",
            "|OrderID|CustomerID|Product|   Category|Quantity|  Price| OrderDate|TotalAmount|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+\n",
            "|      1|       101| Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|\n",
            "|      3|       102| Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|\n",
            "|      7|       102|  Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Standardize the City field in customers_df (e.g., lowercase).\n",
        "from pyspark.sql.functions import lower\n",
        "\n",
        "customers_df = customers_df.withColumn(\"City\",lower(customers_df[\"City\"]))\n",
        "customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48BUq1VLNSt",
        "outputId": "dff16153-6763-415f-a69b-af6f041567d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----------------+---------+----------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|\n",
            "+----------+-----+-----------------+---------+----------+\n",
            "|       101|  Ali|    ali@gmail.com|   mumbai|2022-05-10|\n",
            "|       102| Neha|   neha@yahoo.com|    delhi|2023-01-15|\n",
            "|       103| Ravi| ravi@hotmail.com|bangalore|2021-11-01|\n",
            "|       104|Sneha|sneha@outlook.com|hyderabad|2020-07-22|\n",
            "|       105| Amit|   amit@gmail.com|  chennai|2023-03-10|\n",
            "+----------+-----+-----------------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Extract year from OrderDate and add a new column OrderYear .\n",
        "from pyspark.sql.functions import year, to_date\n",
        "\n",
        "orders_df = orders_df.withColumn(\"OrderDate\",to_date(orders_df[\"OrderDate\"]))\n",
        "orders_df.show()\n",
        "\n",
        "orders_df = orders_df.withColumn(\"OrderYear\",year(orders_df[\"OrderDate\"]))\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T9O1GEHL9ki",
        "outputId": "eac7ce31-42ad-4207-fecd-7bdb9c9d1ffa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Fill null values in any column of your choice with defaults.\n",
        "customers_df = customers_df.fillna({\"Name\":\"Name not provided\"})\n",
        "customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYzkIx51NpBG",
        "outputId": "2a2101cb-7ea8-4a42-eb51-09d176a4bc0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----------------+---------+----------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|\n",
            "+----------+-----+-----------------+---------+----------+\n",
            "|       101|  Ali|    ali@gmail.com|   mumbai|2022-05-10|\n",
            "|       102| Neha|   neha@yahoo.com|    delhi|2023-01-15|\n",
            "|       103| Ravi| ravi@hotmail.com|bangalore|2021-11-01|\n",
            "|       104|Sneha|sneha@outlook.com|hyderabad|2020-07-22|\n",
            "|       105| Amit|   amit@gmail.com|  chennai|2023-03-10|\n",
            "+----------+-----+-----------------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Use when/otherwise to categorize orders:\n",
        "# <5000 : \"Low\"\n",
        "# 5000-20000 : \"Medium\"\n",
        "# >20000 : \"High\"\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "orders_df = orders_df.withColumn(\"OrderCategory\",when(orders_df[\"TotalAmount\"]<5000,\"Low\")\n",
        "                                 .when((orders_df[\"TotalAmount\"] >= 5000) & (orders_df[\"TotalAmount\"] <= 20000),\"Medium\")\n",
        "                                 .otherwise(\"High\"))\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgXLR0GNONT2",
        "outputId": "ba50cbc4-b513-4ece-a218-f2cae3e8d2a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+-------------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderCategory|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+-------------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|         High|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|          Low|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|       Medium|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|          Low|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|       Medium|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|          Low|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|         High|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION B: Spark SQL Tasks\n",
        "# 7. Run a SQL query to list all orders made by “Ali”.\n",
        "\n",
        "spark.sql(\"\"\"select o.orderid,o.category,c.name\n",
        "from sales.orders o join sales.customers c\n",
        "on o.customerid = c.customerid\n",
        "where c.name = 'Ali'\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLLT3PM2QSGk",
        "outputId": "1ed3a7c4-ef84-41c2-cc44-3c619deff9ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----+\n",
            "|orderid|   category|name|\n",
            "+-------+-----------+----+\n",
            "|      1|Electronics| Ali|\n",
            "|      2|Electronics| Ali|\n",
            "+-------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Get total spending by each customer using SQL.\n",
        "\n",
        "spark.sql(\"\"\"select c.name, sum(o.price * o.quantity) as totalspending\n",
        "from sales.orders o join sales.customers c\n",
        "on o.customerid = c.customerid\n",
        "group by c.name\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dudiKkFORYmy",
        "outputId": "057a0944-d174-4dcd-f368-491944dcc40b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "| name|totalspending|\n",
            "+-----+-------------+\n",
            "| Neha|      50000.0|\n",
            "|  Ali|     101200.0|\n",
            "| Ravi|       3500.0|\n",
            "|Sneha|       5000.0|\n",
            "| Amit|       2500.0|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Find out which category made the highest total revenue.\n",
        "\n",
        "spark.sql(\"\"\"select category,sum(price*quantity) as Revenuce from sales.orders group by category order by Revenuce desc limit 1\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ffIuOQSnAT",
        "outputId": "62a9777f-455d-465b-9a01-c5a1216b55e7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+\n",
            "|   category|Revenuce|\n",
            "+-----------+--------+\n",
            "|Electronics|151200.0|\n",
            "+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Create a view customer_orders showing CustomerName, Product, TotalAmount .\n",
        "\n",
        "spark.sql(\"\"\"create or replace temp view customer_orders as\n",
        "select c.name,o.product,o.quantity*o.price as TotalAmount\n",
        "from sales.orders o join sales.customers c\n",
        "on o.customerid = c.customerid\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djb76RnKTNkf",
        "outputId": "1bbf3493-a1a2-4858-dc94-be1ac5685a98"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Query the view for products ordered after Feb 2024.\n",
        "spark.sql(\"\"\"select o.product,c.name\n",
        "from sales.orders o join sales.customers c\n",
        "on o.customerid = c.customerid where o.OrderDate > '2024-02-29'\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwj4hKhGViLK",
        "outputId": "7354e2fd-887a-43f2-abe0-32db901133b3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----+\n",
            "| product|name|\n",
            "+--------+----+\n",
            "|Notebook|Amit|\n",
            "|   Phone|Neha|\n",
            "+--------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION C: Advanced Practice\n",
        "# 12. Create a Global Temp View from customers_df , then query it using:\n",
        "# SELECT * FROM global_temp.customers WHERE City = 'Mumbai';\n",
        "\n",
        "customers_df.createOrReplaceGlobalTempView(\"customers\")\n",
        "\n",
        "spark.sql(\"\"\"SELECT * FROM global_temp.customers WHERE City = 'Mumbai'\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM7IXO03WZIC",
        "outputId": "0f01907e-39d1-40ef-a2df-2cccdc9f4156"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+----+----------+\n",
            "|CustomerID|Name|Email|City|SignupDate|\n",
            "+----------+----+-----+----+----------+\n",
            "+----------+----+-----+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Save the transformed orders_df (with TotalAmount) to a Parquet file.\n",
        "orders_df = orders_df.withColumn(\"TotalAmount\",orders_df[\"Price\"]*orders_df[\"Quantity\"])\n",
        "orders_df.write.mode(\"overwrite\").parquet(\"/tmp/transformed_orders\")"
      ],
      "metadata": {
        "id": "hyHDHnrXXuVR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Read back the Parquet file and count how many orders are in it.\n",
        "orders_parquet = spark.read.parquet(\"/tmp/transformed_orders\")\n",
        "print(orders_parquet.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9BFfLAaZKgT",
        "outputId": "a2a47ae2-8727-45d6-a7cf-d74f14ba5b63"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION D: UDF + Built-in Function Tasks\n",
        "# 15. Write a UDF that masks emails like: ali@gmail.com → a***@gmail.com .\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def mask_email(email):\n",
        "    try:\n",
        "        local, domain = email.split(\"@\")\n",
        "        if len(local) > 1:\n",
        "            return local[0] + \"***@\" + domain\n",
        "        else:\n",
        "            return email\n",
        "    except Exception:\n",
        "        return email\n",
        "\n",
        "mask_email_udf = udf(mask_email, StringType())\n",
        "\n",
        "customers_df = customers_df.withColumn(\"MaskedEmail\", mask_email_udf(\"Email\"))\n",
        "customers_df.select(\"Email\", \"MaskedEmail\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcSsq9CFZoC_",
        "outputId": "e95dea02-2c59-4f11-b8ee-f18a8541bbce"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+\n",
            "|            Email|     MaskedEmail|\n",
            "+-----------------+----------------+\n",
            "|    ali@gmail.com|  a***@gmail.com|\n",
            "|   neha@yahoo.com|  n***@yahoo.com|\n",
            "| ravi@hotmail.com|r***@hotmail.com|\n",
            "|sneha@outlook.com|s***@outlook.com|\n",
            "|   amit@gmail.com|  a***@gmail.com|\n",
            "+-----------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Use concat_ws() to create a full label like: 'Ali from Mumbai' .\n",
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "customers_df = customers_df.withColumn(\"Label\", concat_ws(\" from \", customers_df[\"Name\"], customers_df[\"City\"]))\n",
        "customers_df.select(\"Name\", \"City\", \"Label\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfzp1R1oclU_",
        "outputId": "25789243-7cb1-4ffc-db4d-a29dad12a56c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+--------------------+\n",
            "| Name|     City|               Label|\n",
            "+-----+---------+--------------------+\n",
            "|  Ali|   mumbai|     Ali from mumbai|\n",
            "| Neha|    delhi|     Neha from delhi|\n",
            "| Ravi|bangalore| Ravi from bangalore|\n",
            "|Sneha|hyderabad|Sneha from hyderabad|\n",
            "| Amit|  chennai|   Amit from chennai|\n",
            "+-----+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Use regexp_replace() to remove special characters from product names.\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "orders_df = orders_df.withColumn(\"CleanProduct\", regexp_replace(\"Product\", r\"[^a-zA-Z0-9 ]\", \"\"))\n",
        "orders_df.select(\"Product\", \"CleanProduct\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633oz7M_cxUU",
        "outputId": "36ae6d14-687a-4801-fd57-a9f58dbd5de6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|  Product|CleanProduct|\n",
            "+---------+------------+\n",
            "|   Laptop|      Laptop|\n",
            "|    Mouse|       Mouse|\n",
            "|   Tablet|      Tablet|\n",
            "|Bookshelf|   Bookshelf|\n",
            "|    Mixer|       Mixer|\n",
            "| Notebook|    Notebook|\n",
            "|    Phone|       Phone|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Use to_date() and datediff() to calculate customer age in days (from SignupDate to today).\n",
        "from pyspark.sql.functions import to_date, current_date, datediff\n",
        "\n",
        "customers_df = customers_df.withColumn(\"SignupDate\", to_date(\"SignupDate\")) \\\n",
        "                           .withColumn(\"CustomerAgeDays\", datediff(current_date(), \"SignupDate\"))\n",
        "customers_df.select(\"Name\", \"SignupDate\", \"CustomerAgeDays\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdD6y-T3c8PG",
        "outputId": "de44775c-9511-4f4a-a935-cb4e7b092560"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+---------------+\n",
            "| Name|SignupDate|CustomerAgeDays|\n",
            "+-----+----------+---------------+\n",
            "|  Ali|2022-05-10|           1121|\n",
            "| Neha|2023-01-15|            871|\n",
            "| Ravi|2021-11-01|           1311|\n",
            "|Sneha|2020-07-22|           1778|\n",
            "| Amit|2023-03-10|            817|\n",
            "+-----+----------+---------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}