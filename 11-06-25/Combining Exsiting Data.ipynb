{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d717f6-7210-4d98-9977-58a3bdc76bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1407818079788496#setting/sparkui/0611-043343-dvbo736r/driver-1379547924106414449\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x700d226e4dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"combining exsiting data\").getOrCreate()\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756275f2-8b23-4fb0-9f6b-b070d9892871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------+------+---------+\n|EmpID|  Name|       Dept|  JoinDate|Salary|ManagerID|\n+-----+------+-----------+----------+------+---------+\n|    1|Ananya|         HR|2021-05-01| 55000|     NULL|\n|    2| Rahul|Engineering|2020-03-15| 80000|        1|\n|    3| Priya|Engineering|2022-07-10| 75000|        1|\n|    4|  Zoya|  Marketing|2019-11-20| 60000|        1|\n|    5| Karan|         HR|2023-01-05| 50000|        1|\n|    6|Naveen|Engineering|2021-08-01| 82000|        1|\n|    7|Fatima|  Marketing|2022-09-15| 57000|        1|\n+-----+------+-----------+----------+------+---------+\n\n+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n+------+----+------+\n\n+------+----------------+-----------+\n|  Name|         Project|HoursWorked|\n+------+----------------+-----------+\n|Ananya|       HR Portal|        120|\n| Rahul|   Data Platform|        200|\n| Priya|   Data Platform|        180|\n|  Zoya|Campaign Tracker|        100|\n| Karan|       HR Portal|        130|\n|Naveen|     ML Pipeline|        220|\n|Fatima|Campaign Tracker|         90|\n+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employee = [\n",
    "    (1, \"Ananya\", \"HR\", \"2021-05-01\", 55000, None),\n",
    "    (2, \"Rahul\", \"Engineering\", \"2020-03-15\", 80000, 1),\n",
    "    (3, \"Priya\", \"Engineering\", \"2022-07-10\", 75000, 1),\n",
    "    (4, \"Zoya\", \"Marketing\", \"2019-11-20\", 60000, 1),\n",
    "    (5, \"Karan\", \"HR\", \"2023-01-05\", 50000, 1),\n",
    "    (6, \"Naveen\", \"Engineering\", \"2021-08-01\", 82000, 1),\n",
    "    (7, \"Fatima\", \"Marketing\", \"2022-09-15\", 57000, 1)\n",
    "]\n",
    "columns = [\"EmpID\", \"Name\", \"Dept\", \"JoinDate\", \"Salary\", \"ManagerID\"]\n",
    "df_emp = spark.createDataFrame(employee, columns)\n",
    "\n",
    "\n",
    "columns = [\"EmpID\", \"Name\", \"Dept\", \"JoinDate\", \"Salary\",\"ManagerID\"]\n",
    "\n",
    "df_emp = spark.createDataFrame(employee, columns)\n",
    "df_emp.show()\n",
    "\n",
    "performance = [\n",
    "(\"Ananya\", 2023, 4.5),\n",
    "(\"Rahul\", 2023, 4.9),\n",
    "(\"Priya\", 2023, 4.3),\n",
    "(\"Zoya\", 2023, 3.8),\n",
    "(\"Karan\", 2023, 4.1),\n",
    "(\"Naveen\", 2023, 4.7),\n",
    "(\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)\n",
    "df_perf.show()\n",
    "\n",
    "project_data = [\n",
    "(\"Ananya\", \"HR Portal\", 120),\n",
    "(\"Rahul\", \"Data Platform\", 200),\n",
    "(\"Priya\", \"Data Platform\", 180),\n",
    "(\"Zoya\", \"Campaign Tracker\", 100),\n",
    "(\"Karan\", \"HR Portal\", 130),\n",
    "(\"Naveen\", \"ML Pipeline\", 220),\n",
    "(\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n",
    "df_proj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b4bae3-a67f-4a66-b670-d6b110af5f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+----------+------+---------+----+------+----------------+-----------+\n|  Name|EmpID|       Dept|  JoinDate|Salary|ManagerID|Year|Rating|         Project|HoursWorked|\n+------+-----+-----------+----------+------+---------+----+------+----------------+-----------+\n|Ananya|    1|         HR|2021-05-01| 55000|     NULL|2023|   4.5|       HR Portal|        120|\n| Priya|    3|Engineering|2022-07-10| 75000|        1|2023|   4.3|   Data Platform|        180|\n| Rahul|    2|Engineering|2020-03-15| 80000|        1|2023|   4.9|   Data Platform|        200|\n|  Zoya|    4|  Marketing|2019-11-20| 60000|        1|2023|   3.8|Campaign Tracker|        100|\n| Karan|    5|         HR|2023-01-05| 50000|        1|2023|   4.1|       HR Portal|        130|\n|Naveen|    6|Engineering|2021-08-01| 82000|        1|2023|   4.7|     ML Pipeline|        220|\n|Fatima|    7|  Marketing|2022-09-15| 57000|        1|2023|   3.9|Campaign Tracker|         90|\n+------+-----+-----------+----------+------+---------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Joins and Advanced Aggregations\n",
    "# 1. Join employee_data , performance_data , and project_data .\n",
    "\n",
    "df_joined = df_emp.join(df_perf, on=\"Name\", how=\"inner\").join(df_proj, on=\"Name\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8ea085-c238-42d9-8514-e2b10772730e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n|       Dept|sum(HoursWorked)|\n+-----------+----------------+\n|         HR|             250|\n|Engineering|             600|\n|  Marketing|             190|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute total hours worked per department.\n",
    "\n",
    "df_joined.groupBy(\"Dept\").agg({\"HoursWorked\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9524c62-6670-4219-a7a0-fb9234829b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|         Project|       avg(Rating)|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute average rating per project.\n",
    "\n",
    "df_joined.groupBy(\"Project\").agg({\"Rating\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29cee2e-fe51-4986-a444-f50eb3fc0130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n|   Name|Year|Rating|\n+-------+----+------+\n| Ananya|2023|   4.5|\n|  Rahul|2023|   4.9|\n|  Priya|2023|   4.3|\n|   Zoya|2023|   3.8|\n|  Karan|2023|   4.1|\n| Naveen|2023|   4.7|\n| Fatima|2023|   3.9|\n|Mariyam|2023|  NULL|\n+-------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Data (introduce some manually)\n",
    "# 4. Add a row to performance_data with a None rating.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "new_row = [(\"Mariyam\", 2023, None)]\n",
    "\n",
    "new_perf = spark.createDataFrame(new_row, df_perf.schema)\n",
    "\n",
    "df_perf_null = df_perf.union(new_perf)\n",
    "df_perf_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4f9d3a-8690-4e3b-9ffd-803cd863d061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n|   Name|Year|Rating|\n+-------+----+------+\n|Mariyam|2023|  NULL|\n+-------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Filter rows with null values.\n",
    "\n",
    "df_perf_null.filter(df_perf_null[\"Rating\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5992bdd0-df8b-4814-be88-0e5ccf948892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+-------------+\n|   Name|       Dept|Rating|Rating_Filled|\n+-------+-----------+------+-------------+\n| Ananya|         HR|   4.5|          4.5|\n|  Rahul|Engineering|   4.9|          4.9|\n|  Priya|Engineering|   4.3|          4.3|\n|   Zoya|  Marketing|   3.8|          3.8|\n|  Karan|         HR|   4.1|          4.1|\n| Naveen|Engineering|   4.7|          4.7|\n| Fatima|  Marketing|   3.9|          3.9|\n|Mariyam|       NULL|  NULL|         NULL|\n+-------+-----------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Replace null ratings with the department average.\n",
    "\n",
    "from pyspark.sql.functions import avg, when, col\n",
    "\n",
    "df_perf_with_dept = df_perf_null.join(df_emp.select(\"Name\", \"Dept\"), on=\"Name\", how=\"left\")\n",
    "\n",
    "dept_avg = df_perf_with_dept.filter(col(\"Rating\").isNotNull()).groupBy(\"Dept\").agg(avg(\"Rating\").alias(\"DeptAvg\"))\n",
    "\n",
    "df_with_avg = df_perf_with_dept.join(dept_avg, on=\"Dept\", how=\"left\")\n",
    "\n",
    "df_filled = df_with_avg.withColumn(\"Rating_Filled\",when(col(\"Rating\").isNull(), col(\"DeptAvg\")).otherwise(col(\"Rating\")))\n",
    "\n",
    "df_filled.select(\"Name\", \"Dept\", \"Rating\", \"Rating_Filled\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbcbef8a-59ce-4499-9d8d-34f928651c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----+------+------------------+-------------+-------------------+\n|       Dept|   Name|Year|Rating|           DeptAvg|Rating_Filled|PerformanceCategory|\n+-----------+-------+----+------+------------------+-------------+-------------------+\n|         HR| Ananya|2023|   4.5|               4.3|          4.5|               Good|\n|Engineering|  Rahul|2023|   4.9| 4.633333333333334|          4.9|          Excellent|\n|Engineering|  Priya|2023|   4.3| 4.633333333333334|          4.3|               Good|\n|  Marketing|   Zoya|2023|   3.8|3.8499999999999996|          3.8|            Average|\n|         HR|  Karan|2023|   4.1|               4.3|          4.1|               Good|\n|Engineering| Naveen|2023|   4.7| 4.633333333333334|          4.7|          Excellent|\n|  Marketing| Fatima|2023|   3.9|3.8499999999999996|          3.9|            Average|\n|       NULL|Mariyam|2023|  NULL|              NULL|         NULL|            Average|\n+-----------+-------+----+------+------------------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Built-In Functions and UDF\n",
    "# 7. Create a column PerformanceCategory :\n",
    "# Excellent (>=4.7),\n",
    "# Good (4.0–4.69),\n",
    "# Average (<4.0)\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_with_category = df_filled.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(col(\"Rating_Filled\") >= 4.7, \"Excellent\")\n",
    "    .when((col(\"Rating_Filled\") >= 4.0) & (col(\"Rating_Filled\") < 4.7), \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea77b50-898e-491b-884f-3dd97e03e934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+------+------------------+-------------+-------------------+----------------+-----------+-----+\n|   Name|       Dept|Year|Rating|           DeptAvg|Rating_Filled|PerformanceCategory|         Project|HoursWorked|Bonus|\n+-------+-----------+----+------+------------------+-------------+-------------------+----------------+-----------+-----+\n| Ananya|         HR|2023|   4.5|               4.3|          4.5|               Good|       HR Portal|        120| 5000|\n|  Rahul|Engineering|2023|   4.9| 4.633333333333334|          4.9|          Excellent|   Data Platform|        200| 5000|\n|  Priya|Engineering|2023|   4.3| 4.633333333333334|          4.3|               Good|   Data Platform|        180| 5000|\n|   Zoya|  Marketing|2023|   3.8|3.8499999999999996|          3.8|            Average|Campaign Tracker|        100| 5000|\n|  Karan|         HR|2023|   4.1|               4.3|          4.1|               Good|       HR Portal|        130| 5000|\n| Naveen|Engineering|2023|   4.7| 4.633333333333334|          4.7|          Excellent|     ML Pipeline|        220|10000|\n| Fatima|  Marketing|2023|   3.9|3.8499999999999996|          3.9|            Average|Campaign Tracker|         90| 5000|\n|Mariyam|       NULL|2023|  NULL|              NULL|         NULL|            Average|            NULL|       NULL|    0|\n+-------+-----------+----+------+------------------+-------------+-------------------+----------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. Create a UDF to assign bonus:\n",
    "# If project hours > 200 → 10,000 Else → 5,000\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_final = df_with_category.join(df_proj, on=\"Name\", how=\"left\")\n",
    "\n",
    "def assign_bonus(hours):\n",
    "    if hours is None:\n",
    "        return 0\n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus_udf = udf(assign_bonus, IntegerType())\n",
    "\n",
    "df_final = df_final.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\")))\n",
    "\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d3815d-26dc-408a-be1c-cb4da2ff927f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------+------+---------+------------+\n|EmpID|  Name|       Dept|  JoinDate|Salary|ManagerID|MonthsWorked|\n+-----+------+-----------+----------+------+---------+------------+\n|    1|Ananya|         HR|2021-06-01| 55000|     NULL|          48|\n|    2| Rahul|Engineering|2021-06-01| 80000|        1|          48|\n|    3| Priya|Engineering|2021-06-01| 75000|        1|          48|\n|    4|  Zoya|  Marketing|2021-06-01| 60000|        1|          48|\n|    5| Karan|         HR|2021-06-01| 50000|        1|          48|\n|    6|Naveen|Engineering|2021-06-01| 82000|        1|          48|\n|    7|Fatima|  Marketing|2021-06-01| 57000|        1|          48|\n+-----+------+-----------+----------+------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Date and Time Functions\n",
    "# 9. Add a column JoinDate with 2021-06-01 for all, then add MonthsWorked as\n",
    "# difference from today.\n",
    "\n",
    "from pyspark.sql.functions import lit, to_date, current_date, months_between\n",
    "\n",
    "df_with_dates = df_emp.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
    "\n",
    "df_with_dates = df_with_dates.withColumn(\n",
    "    \"MonthsWorked\",\n",
    "    months_between(current_date(), col(\"JoinDate\")).cast(\"int\")\n",
    ")\n",
    "\n",
    "df_with_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676ef857-1540-4f2c-916e-41df8cb20f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|  Name|  JoinDate|\n+------+----------+\n|Ananya|2021-06-01|\n| Rahul|2021-06-01|\n| Priya|2021-06-01|\n|  Zoya|2021-06-01|\n| Karan|2021-06-01|\n|Naveen|2021-06-01|\n|Fatima|2021-06-01|\n+------+----------+\n\nTotal joined before 2022: 7\n"
     ]
    }
   ],
   "source": [
    "# 10. Calculate how many employees joined before 2022.\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_joined_before_2022 = df_with_dates.filter(col(\"JoinDate\") < to_date(lit(\"2022-01-01\")))\n",
    "\n",
    "df_joined_before_2022.select(\"Name\", \"JoinDate\").show()\n",
    "\n",
    "print(\"Total joined before 2022:\", df_joined_before_2022.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa6311f-71be-4a0f-b901-b6625bcb2c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------+------+---------+\n|EmpID|  Name|       Dept|  JoinDate|Salary|ManagerID|\n+-----+------+-----------+----------+------+---------+\n|    1|Ananya|         HR|2021-05-01| 55000|     NULL|\n|    2| Rahul|Engineering|2020-03-15| 80000|        1|\n|    3| Priya|Engineering|2022-07-10| 75000|        1|\n|    4|  Zoya|  Marketing|2019-11-20| 60000|        1|\n|    5| Karan|         HR|2023-01-05| 50000|        1|\n|    6|Naveen|Engineering|2021-08-01| 82000|        1|\n|    7|Fatima|  Marketing|2022-09-15| 57000|        1|\n|    8| Meena|         HR|      NULL| 48000|     NULL|\n|    9|   Raj|  Marketing|      NULL| 51000|     NULL|\n+-----+------+-----------+----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Unions\n",
    "# 11. Create another small team DataFrame and union() it with employee_data .\n",
    "# extra_employees = [\n",
    "# (\"Meena\", \"HR\", 48000),\n",
    "# (\"Raj\", \"Marketing\", 51000)\n",
    "# ]\n",
    "\n",
    "from pyspark.sql import Row\n",
    "extra_employees = [\n",
    "(8,\"Meena\", \"HR\", None,48000,None),\n",
    "(9,\"Raj\", \"Marketing\",None, 51000,None)\n",
    "]\n",
    "new_emp_df = spark.createDataFrame(extra_employees, df_emp.schema)\n",
    "df_emp = df_emp.union(new_emp_df)\n",
    "\n",
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83b5029-4abc-4dd6-baab-806ae8a6be4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving Results\n",
    "# 12. Save the final merged dataset (all 3 joins) as a partitioned Parquet file based on Department .\n",
    "\n",
    "df_final = df_emp.join(df_perf_null, on=\"Name\", how=\"inner\").join(df_proj, on=\"Name\", how=\"inner\")\n",
    "\n",
    "df_final.write.partitionBy(\"Dept\").mode(\"overwrite\").parquet(\"/tmp/merged_data_partitioned_by_dept\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Combining Exsiting Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}