{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca68908d-16e4-47e7-9dd9-1d45a65ac8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1407818079788496#setting/sparkui/0611-043343-dvbo736r/driver-1379547924106414449\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7540d742dd50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"agg and grouping\").getOrCreate()\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df30e33-5148-4f61-b74c-15cec71a5a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------+------+---------+\n|EmpID|  Name|       Dept|  JoinDate|Salary|ManagerID|\n+-----+------+-----------+----------+------+---------+\n|    1| Anita|         HR|2021-05-01| 55000|     NULL|\n|    2|   Raj|Engineering|2020-03-15| 80000|        1|\n|    3|Simran|Engineering|2022-07-10| 75000|        1|\n|    4| Aamir|  Marketing|2019-11-20| 60000|        1|\n|    5| Nisha|         HR|2023-01-05| 50000|        1|\n+-----+------+-----------+----------+------+---------+\n\n+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n| Anita|2023|   4.5|\n|   Raj|2023|   4.9|\n|Simran|2023|   4.3|\n| Aamir|2023|   3.8|\n| Karan|2023|   4.1|\n| Nisha|2023|   4.7|\n|Fatima|2023|   3.9|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee = [\n",
    "    (1, \"Anita\", \"HR\", \"2021-05-01\", 55000,None),\n",
    "    (2, \"Raj\", \"Engineering\", \"2020-03-15\", 80000,1),\n",
    "    (3, \"Simran\", \"Engineering\", \"2022-07-10\", 75000,1),\n",
    "    (4, \"Aamir\", \"Marketing\", \"2019-11-20\", 60000,1),\n",
    "    (5, \"Nisha\", \"HR\", \"2023-01-05\", 50000,1)\n",
    "]\n",
    "\n",
    "columns = [\"EmpID\", \"Name\", \"Dept\", \"JoinDate\", \"Salary\",\"ManagerID\"]\n",
    "\n",
    "df_emp = spark.createDataFrame(employee, columns)\n",
    "df_emp.show()\n",
    "\n",
    "performance = [\n",
    "(\"Anita\", 2023, 4.5),\n",
    "(\"Raj\", 2023, 4.9),\n",
    "(\"Simran\", 2023, 4.3),\n",
    "(\"Aamir\", 2023, 3.8),\n",
    "(\"Karan\", 2023, 4.1),\n",
    "(\"Nisha\", 2023, 4.7),\n",
    "(\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)\n",
    "df_perf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfd8eba-adb7-44cb-a9d9-3be625b3ee96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|       Dept|avg(Salary)|\n+-----------+-----------+\n|         HR|    52500.0|\n|Engineering|    77500.0|\n|  Marketing|    60000.0|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations\n",
    "# 1. Get the average salary by department.\n",
    "\n",
    "df_emp.groupBy(\"Dept\").agg({\"Salary\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b618f5-f6d2-4d13-a0f7-7c48bece87df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|       Dept|count(EmpID)|\n+-----------+------------+\n|         HR|           2|\n|Engineering|           2|\n|  Marketing|           1|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Count of employees per department.\n",
    "\n",
    "df_emp.groupBy(\"Dept\").agg({\"EmpID\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e2b65d-306f-4cfe-9818-707a9b4dc582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|Max_Salary|Min_Salary|\n+----------+----------+\n|     80000|     75000|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Maximum and minimum salary in Engineering.\n",
    "\n",
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "df_emp.filter(df_emp.Dept == \"Engineering\").agg(max(\"Salary\").alias(\"Max_Salary\"),min(\"Salary\").alias(\"Min_Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5ee958-fffb-4dd1-99c9-23329a7c673d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+----------+------+---------+----+------+\n|  Name|EmpID|       Dept|  JoinDate|Salary|ManagerID|Year|Rating|\n+------+-----+-----------+----------+------+---------+----+------+\n| Aamir|    4|  Marketing|2019-11-20| 60000|        1|2023|   3.8|\n| Anita|    1|         HR|2021-05-01| 55000|     NULL|2023|   4.5|\n| Nisha|    5|         HR|2023-01-05| 50000|        1|2023|   4.7|\n|   Raj|    2|Engineering|2020-03-15| 80000|        1|2023|   4.9|\n|Simran|    3|Engineering|2022-07-10| 75000|        1|2023|   4.3|\n+------+-----+-----------+----------+------+---------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Join and Combine Data\n",
    "# 4. Perform an inner join between employee_data and performance_data on Name .\n",
    "\n",
    "df_joined = df_emp.join(df_perf, on=\"Name\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1105d6-1199-4d7f-bd40-3c604d07461b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n| Aamir| 60000|   3.8|\n| Anita| 55000|   4.5|\n| Nisha| 50000|   4.7|\n|   Raj| 80000|   4.9|\n|Simran| 75000|   4.3|\n+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Show each employeeâ€™s salary and performance rating.\n",
    "\n",
    "df_joined.select(\"Name\", \"Salary\", \"Rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443d6eb6-2504-45b2-8c25-73f72b8a80c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+------+---------+----+------+\n|Name|EmpID|       Dept|  JoinDate|Salary|ManagerID|Year|Rating|\n+----+-----+-----------+----------+------+---------+----+------+\n| Raj|    2|Engineering|2020-03-15| 80000|        1|2023|   4.9|\n+----+-----+-----------+----------+------+---------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Filter employees with rating > 4.5 and salary > 60000.\n",
    "\n",
    "df_joined.filter((df_joined.Rating > 4.5) & (df_joined.Salary > 60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110c15cd-a727-4037-a12d-22f3f1d4b047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+------+-----------+\n|EmpID|  Name|       Dept|Salary|Salary_Rank|\n+-----+------+-----------+------+-----------+\n|    2|   Raj|Engineering| 80000|          1|\n|    3|Simran|Engineering| 75000|          2|\n|    1| Anita|         HR| 55000|          1|\n|    5| Nisha|         HR| 50000|          2|\n|    4| Aamir|  Marketing| 60000|          1|\n+-----+------+-----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Window & Rank (Bonus Challenge)\n",
    "# 7. Rank employees by salary department-wise.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,sum\n",
    "\n",
    "window_rank = Window.partitionBy(\"Dept\").orderBy(df_emp[\"Salary\"].desc())\n",
    "\n",
    "df_emp.withColumn(\"Salary_Rank\", rank().over(window_rank)).select(\"EmpID\", \"Name\", \"Dept\", \"Salary\", \"Salary_Rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66a5142-965d-45c6-8bc7-1d8c3f4f9c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+------+-----------------+\n|EmpID|  Name|       Dept|Salary|Cumulative_Salary|\n+-----+------+-----------+------+-----------------+\n|    3|Simran|Engineering| 75000|            75000|\n|    2|   Raj|Engineering| 80000|           155000|\n|    5| Nisha|         HR| 50000|            50000|\n|    1| Anita|         HR| 55000|           105000|\n|    4| Aamir|  Marketing| 60000|            60000|\n+-----+------+-----------+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. Calculate cumulative salary in each department.\n",
    "window_cumsum = Window.partitionBy(\"Dept\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_emp.withColumn(\"Cumulative_Salary\", sum(\"Salary\").over(window_cumsum)).select(\"EmpID\", \"Name\", \"Dept\", \"Salary\", \"Cumulative_Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3def6834-ec68-4383-b906-927721c42537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+------+---------+----------+\n|EmpID|  Name|       Dept|Salary|ManagerID|  JoinDate|\n+-----+------+-----------+------+---------+----------+\n|    1| Anita|         HR| 55000|     NULL|2021-11-02|\n|    2|   Raj|Engineering| 80000|        1|2021-08-25|\n|    3|Simran|Engineering| 75000|        1|2020-05-13|\n|    4| Aamir|  Marketing| 60000|        1|2020-03-14|\n|    5| Nisha|         HR| 50000|        1|2020-04-22|\n+-----+------+-----------+------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Date Operations\n",
    "# 9. Add a new column JoinDate with random dates between 2020 and 2023.\n",
    "\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def random_date():\n",
    "    start_date = date(2020, 1, 1)\n",
    "    end_date = date(2023, 12, 31)\n",
    "    delta = end_date - start_date\n",
    "    return str(start_date + timedelta(days=random.randint(0, delta.days)))\n",
    "\n",
    "random_dates = [random_date() for _ in range(df_emp.count())]\n",
    "\n",
    "random_date_df = spark.createDataFrame([(d,) for d in random_dates], [\"RandomJoinDate\"])\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_emp_indexed = df_emp.withColumn(\"id\", monotonically_increasing_id())\n",
    "random_date_indexed = random_date_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "df_emp_random = df_emp_indexed.join(random_date_indexed, \"id\") \\\n",
    "    .drop(\"JoinDate\") \\\n",
    "    .withColumnRenamed(\"RandomJoinDate\", \"JoinDate\") \\\n",
    "    .withColumn(\"JoinDate\", to_date(\"JoinDate\", \"yyyy-MM-dd\")) \\\n",
    "    .drop(\"id\")\n",
    "\n",
    "df_emp_random.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30e0508-2f74-4232-aa46-b588e49a146a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+\n|  Name|  JoinDate|YearsWithCompany|\n+------+----------+----------------+\n| Anita|2021-11-02|             3.6|\n|   Raj|2021-08-25|             3.8|\n|Simran|2020-05-13|             5.1|\n| Aamir|2020-03-14|             5.2|\n| Nisha|2020-04-22|             5.1|\n+------+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Add column YearsWithCompany using current_date() and datediff() .\n",
    "\n",
    "from pyspark.sql.functions import current_date, datediff, round\n",
    "\n",
    "df_emp_years = df_emp_random.withColumn(\"YearsWithCompany\",round(datediff(current_date(), \"JoinDate\") / 365, 1))\n",
    "\n",
    "df_emp_years.select(\"Name\", \"JoinDate\", \"YearsWithCompany\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b253cc66-64a3-4c4e-b64d-1cce82acb9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing to Files\n",
    "# 11. Write the full employee DataFrame to CSV with headers.\n",
    "df_emp_random.write.mode('overwrite').csv(\"/FileStore/tables/employee_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a16db9e-9d09-4299-9d38-afd19cb5f2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 12. Save the joined DataFrame to a Parquet file.\n",
    "df_joined.write.mode('overwrite').parquet(\"/FileStore/tables/employee_performance.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05364d7-aebf-4801-9d26-2488243591d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[EmpID: int, Name: string, Dept: string, Salary: int, ManagerID: int, JoinDate: date]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(\"/FileStore/tables/employee_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b399b7d8-106d-4636-aa34-978aef300f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Name: string, EmpID: bigint, Dept: string, JoinDate: string, Salary: bigint, ManagerID: bigint, Year: bigint, Rating: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"/FileStore/tables/employee_performance.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggr and Grouping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}